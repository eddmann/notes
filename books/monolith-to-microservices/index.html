<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Notes</title><link href=https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css rel=stylesheet><style>h1,h2,h3,h4,h5,h6{border:0;margin:0;padding:0}h1{padding:1.6rem 0 1.4rem 0;font-size:1.802em;line-height:1.2;font-weight:700}h2{padding:1.6rem 0 1.2rem;font-size:1.602em;line-height:1.2;font-weight:600}h3{padding:1.5rem 0 1rem 0;font-size:1.424em;line-height:1.3;font-weight:600}h4{padding:1.2rem 0 .8rem 0;font-size:1.266em;line-height:1.4;font-weight:600}h5{padding:.8rem 0 .4rem 0;font-size:1em;line-height:1.5;font-weight:600}strong{font-weight:600}header{display:flex;align-items:center;gap:1rem}blockquote{padding:1rem}</style><body><header><h1>üìù Notes</h1><nav><a href=https://notes.eddmann.com>Listing</a> / <a href=https://eddmann.com>Blog</a></nav></header><h1>Monolith to Microservices</h1><p>by <strong>Sam Newman</strong>, 2019 <a href=https://www.oreilly.com/library/view/monolith-to-microservices/9781492047834/ target=_blank>ref</a><details><summary>Table of Contents</summary> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#just-enough-microservices>Just Enough Microservices</a> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#microservices>Microservices...</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#monoliths>Monoliths...</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#coupling-and-cohesion>Coupling and Cohesion</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#domain-driven-design>Domain-Driven Design</a></ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#planning-a-migration>Planning a Migration</a> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#key-questions>Key Questions</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#microservices-could-be-a-good-choice-to>Microservices could be a good choice to...</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#microservices-could-be-a-bad-choice-when-you>Microservices could be a bad choice when you...</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#importance-of-incremental-migration>Importance of Incremental Migration</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#cost-of-change>Cost of Change</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#using-a-domain-model>Using a Domain Model</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#reorganising-teams>Reorganising Teams</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#how-will-you-know-if-the-transition-is-working>How Will You Know if the Transition Is Working?</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#avoiding-the-sunk-cost-fallacy>Avoiding the Sunk Cost Fallacy</a></ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#splitting-the-monolith>Splitting the Monolith</a> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#refactoring-the-monolith>Refactoring the Monolith</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-strangler-fig-application>Pattern: Strangler Fig Application</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-ui-composition>Pattern: UI Composition</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-branch-by-abstraction>Pattern: Branch by Abstraction</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-parallel-run>Pattern: Parallel Run</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-decorating-collaborator>Pattern: Decorating Collaborator</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-change-data-capture>Pattern: Change Data Capture</a></ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#decomposing-the-database>Decomposing the Database</a> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-database-view>Pattern: Database View</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-database-wrapping-service>Pattern: Database Wrapping Service</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-database-as-a-service-interface>Pattern: Database-as-a-Service Interface</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-aggregate-exposing-monolith>Pattern: Aggregate Exposing Monolith</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-change-data-ownership>Pattern: Change Data Ownership</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-synchronise-data-in-application>Pattern: Synchronise Data in Application</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#pattern-tracer-write>Pattern: Tracer Write</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#transactions>Transactions</a></ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#growing-pains>Growing Pains</a> <ul><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#code-ownership-at-scale>(Code) Ownership at Scale</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#breaking-changes>Breaking Changes</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#reporting>Reporting</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#monitoring-and-troubleshooting>Monitoring and Troubleshooting</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#local-developer-experience>Local Developer Experience</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#running-too-many-things>Running Too Many Things</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#end-to-end-testing>End-to-End Testing</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#global-vs-local-optimisation>Global vs. Local Optimisation</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#robustness-and-resiliency>Robustness and Resiliency</a><li><a href=https://notes.eddmann.com/books/monolith-to-microservices/#orphaned-services>Orphaned Services</a></ul></ul></details><h2 id=just-enough-microservices>Just Enough Microservices</h2><blockquote><p>Microservices are independently deployable services modelled around a business domain.</blockquote><p><strong>Independent deployability</strong><p>This allows us to change a microservice and deploy it into production without having to utilise/change any other services. This propriety requires loose coupling ('change one service without having to change anything else'.). We use stable contracts help guide how we find the service boundaries.<p><strong>Modelled around a business domain</strong><blockquote><p>"Any organisation that designs a system‚Ä¶ will inevitably produce a design whose structure is a copy of the organisation's communication structure" - Melvin Conway</blockquote><p>We used to organise people in terms of their core competence, However, changes in functionality are primarily about changes in business functionality. With this grouping (i.e. three-tier architecture with, UI team, Backend team and DBAs) the business functionality is spread across all three teams - increasing the chance that a change will cross layers and require coordination. This is an architecture in which we have high cohesion of related technology, but low cohesion of business functionality.<p>This does not organise people for the change we want to optimise for (business functionality), as such we now wish to group people in poly-skilled teams to reduce hand-offs and silos. Instead our business domain becomes the primary force driving our system architecture. We now have teams which manage vertical (end-to-end) slices of business functionality over horizontal slices of core competence (technology). Its up to the poly-skilled teams how the code is structured, state is stored and how it is presentated. The only external concern is the stable contract for the business functionality exposed.<h3 id=microservices>Microservices...</h3><ul><li>Are a type of SOA and distributed system, opinionated about how services boundaries are drawn and where independent deployability is paramount.<li>Microservice architecture is based on multiple collaborating microservices.<li>Are technology agnostic - as they communicate over a common network protocol, exposing business behaviour over one of more network endpoints via well-defined stable interfaces/contracts.<li>The concept of size is one of the least interesting things about this type of architecture. Measurement of size is a bit ambiguous, could it be SLOC? Instead we should focus on having as 'small an public interface as possible'.<li>We should be focused on service boundaries and how many services we can incrementally handle as a company/team, over the size of each service. It is easier to break something up than put it back together.<li>We want delivery teams aligned around product lines, and the services aligned around the business domain, this makes it easy to assign ownership to these product-oriented delivery teams.<li>Are not just for the backend, we should be considering the UI too (Micro-frontends). This aligns with how we wish to align our teams around end-to-end business functionality.<li>'buy you options' - There is a implied cost to going with this architecture but the cost may be worth the options you get from it.<li>Microservice architectures give you more concrete boundaries in a system around which ownership lines can be drawn.<li>Provide you with: <ul><li>Scale and robustness of the systems and teams implementing them.<li>Services which can be worked on in parallel.<li>Clearer understanding of decomposed parts of the system.<li>Process isolation, which allows for varying technology choices.<li>Isolated deployments - why expand the blast radius and have to re-deploy unrelated functionality each time.<li>Fault isolation - failures in one service are less likely to impact other services, improving the overall resilience of the system.</ul><li>They suffer from: <ul><li>the failure and varying increased latency of network communication<li>distributed architecture headaches - good-bye to single-process transactions</ul><li>Treat the service interfaces that your microservice exposes like a user interface. Use <em>outside-in</em> thinking to shape the interface design in partnership with the people who will call your service.</ul><h3 id=monoliths>Monoliths...</h3><ul><li>Are defined by a unit of deployment, when all functionality in a system has to be deployed together, we consider it a monolith.<li>Are a simple form of distributed system - as it likely reads from an external database and presents data on a clients web browsers. There are three different processes communicating over a network right there. The difference is to the extent in which the system is distributed compared to microservice architectures. More processes/computers, equals more network communication, equals more of a chance to see problems related to distributed systems.<li>Simpler deployment topology can avoid many of the pitfalls associated with distributed systems. It can result in much simpler developer workflows; and monitoring, troubleshooting, and activities like end-to-end testing can be greatly simplified as well.<li>If we want to reuse code within a distributed system, we have to decide whether we want to copy code, break out libraries, or push the shared functionality into a service. With a monolith, our choices are much simpler - all the code is there!<li>Vulnerable to implementation and deployment coupling.<li>Vulnerable to delivery contention (deployment trains) - different developers wanting to change the same piece of code, different teams wanting to push functionality live at different times. Can suffer from confusion around who owns what, and who makes what decision.<li>Most common type is <em>single-process system</em> - in which all of the code is deployed as a single process.<li>Subset of a <em>single-process system</em> monolith is a <strong>modular monolith</strong> - single process consisting of separate modules, each of which can be worked on independently, but which still need to be combined for deployment. Good, often overlooked choice (<a href=https://shopify.engineering/deconstructing-monolith-designing-software-maximizes-developer-productivity rel=noopener target=_blank>Shopify</a> is a good example) - if the module boundaries are well defined, it can allow for a high degree of parallel working, but sidesteps the challenges of distributed microservice architecture along with much simpler deployment concerns. The modular monolith can suffer from challenges around shared state (i.e. with a database) and how it can be decomposed in the future due to shared cross-cutting concerns.<li>A <strong>distributed monolith</strong> is a system that consists of multiple services, but for whatever reason the entire system has to be deployed together. It has all the disadvantages of SOA/distributed systems and all the disadvantages of a monolith. This form of monolith typically occurs when not enough focus was placed on concepts like information hiding and cohesion of business functionality.</ul><h3 id=coupling-and-cohesion>Coupling and Cohesion</h3><p><img alt="Coupling vs Cohesion" src=/assets/books/monolith-to-microservices/cohesion-vs-coupling.png><p><strong>Coupling</strong> refers to the degree of interdependence between <em>modules</em> i.e. a change in one, requires a change in another. Desirable low coupling means that modules in a system have minimal dependence on each other.<p><strong>Cohesion</strong> refers to the degree to which the elements inside a <em>module</em> belong together. Desirable high cohesion means that a module is focused on a single/closely related task(s) - 'the code that changes together, stays together'.<p>There tends to be an inverse relationship with the two forces - as cohesion increases within a module, coupling tends to decrease. The same is true the other way round, if we have two pieces of tightly related code, cohesion is low as the related functionality is spread across both. We also have tight coupling, as when this related code changes, both things need to change.<p>Low coupling, high cohesion is a desirable trait for reduced maintenance and modification cost of a given <em>module</em>.<p>These concepts are really important in distributed system design as getting this balance wrong has a higher cost than within a single-process monolith. We want to optimise our architecture around ease of making changes in business functionality - so we want the functionality grouped in such a way that we can make changes in as few places as possible.<p>Information hiding (aka. decision hiding) is an important design principle to address coupling concerns. The core idea being to separate the parts of the code that change frequently from the ones that are static. We want the <em>module</em> boundary to be stable, and it should hide those parts of the module implementation that we expect to change more often. The smaller the public interface of a module the less chance of coupling with other modules, as there is less for them to 'couple' with.<p>There a several different types of coupling to consider:<ul><li><strong>Implementation coupling</strong> - refers to the degree to which the implementation details of one module are known to, or dependent upon, another module. This type of coupling occurs when a module relies on the specific internal workings of another module, rather than interacting with it through a well-defined interface. A common example of this comes in the form of sharing a database.<li><strong>Temporal coupling</strong> - refers to a runtime situation where the correct execution of one module depends on the timing or sequence of actions performed by another module. It's often linked with a primary challenge encountered in synchronous calls within distributed systems. For example, dependance on a synchronous external module response within your own response to a caller.<li><strong>Deployment coupling</strong> - refers to a situation where the deployment of one module is dependent on the deployment of module. In a single-process monolith with multiple linked modules, altering a line in one module means deploying the entire monolith, even if most modules remain unchanged. Deployments inherently pose risk - but smaller, more focused releases help minimize this risk.<li><strong>Domain coupling</strong> - there is no getting away from some degree of domain coupling, as to make any useful system, <em>modules</em> will have to interact with each other in some form. These interactions between modules model the interactions found in the real domain i.e. if you want to place an order, you need to know what items were in a customer's shopping basket. If you want to ship a product, you need to know where you ship it. In a microservice architecture, this information may be contained in different services, as such they will have to communicate with each other. You must be aware of the level of detail each module/service needs to know about each other to do their work.</ul><h3 id=domain-driven-design>Domain-Driven Design</h3><ul><li>DDD provides us with a means to help model our services around a business domain.<li>You <em>discover</em> sub-domains within a domain and <em>design</em> bounded contexts.<li>We use Event Storming to help shape the domain models with non-developer colleagues.<li>Aggregates... <ul><li>are a transactional boundary.<li>protect their own invariants, encapsulating the behaviour and state required to do so.<li>something that should be considered as a whole - not being decomposable into separate accessible parts, it is the self-contained unit.<li>have a life-cycle around them.<li>can have relationships with other aggregates via <em>aggregate root</em> identity.<li>are designed for many different reasons and can be reshaped over time.<li>are a tactical DDD pattern.<li>self-contained state machine that focuses on a single domain concept in our system.</ul><li>Microservices will handle the life-cycle and data storage or one or more different type of aggregate.<li>Bounded contexts... <ul><li>are a strategic pattern used to design a boundary within/or encompass a sub-domain.<li>encapsulates a specific part of the domain model and ensures that within its boundary, terms and concepts have a single, specific meaning (ubiquitous language).<li>hide implementation details which are internal concerns, providing a public interface to other bounded contexts.<li>can use anti-corruption layers (ACL) to ensure internal consistency when communicating with other bounded contexts.<li>contains one or more aggregates.<li>are the boundary in which a given domain model is deemed valid.</ul><li>Both the aggregate and the bounded context give us units of cohesion with well-defined interfaces to the wider system.<li>When starting out with microservices it is advised to target services which encompass entire bounded contexts. We can later look to decompose these into smaller services, but this is an internal implementation decision. The public API to the consumer does not need to change.</ul><h2 id=planning-a-migration>Planning a Migration</h2><p>Microservices are not the goal! You should be considering migrating to a microservice architecture to achieve capabilities that your current system architecture cannot provide. The decision should be rooted in the challenges you are facing, and the changes you want to bring about.<h3 id=key-questions>Key Questions</h3><ul><li><strong>What are you hoping to achieve?</strong> - set of outcomes aligned with the business's objectives, highlighting the benefits to the system's end users.<li><strong>Have you considered alternatives to using microservices?</strong> - quite often you can get what you need by using a much easier, more <em>boring</em> technique.<li><strong>How will you know if the transition is working?</strong> - we will need regular checkpoints, quantitative and qualitative measures to ensure we are still on the right path. So as to help avoid the sunk cost fallacy.</ul><h3 id=microservices-could-be-a-good-choice-to>Microservices could be a good choice to...</h3><h4 id=improve-team-autonomy>Improve Team Autonomy</h4><p>Many organisations have achieved more effective growth and scaling than their peers by keeping groups small, fostering close bonds, and minimising bureaucracy. Models like Amazon's 'two-pizza team' and Spotify's approach support this strategy. When teams own microservices and have full control over them, they gain greater autonomy and empowerment within the larger organisation.<p><strong>However you could...</strong><p>Assign ownership of different parts of the codebase to various teams, such as through a modular monolith approach. This could also be done by empowering individual team members to make decisions based on their expertise in specific areas of the codebase (e.g., X specialises in display ads, while Y has extensive knowledge of the checkout flow).<h4 id=reduce-time-to-market>Reduce Time to Market</h4><p>By making and deploying changes to individual microservices independently, without waiting for coordinated releases, we can deliver new functionality to our customers more quickly.<p><strong>However you could...</strong><p>Numerous factors affect the speed of software delivery. It is recommended to conduct a path-to-production modelling exercise, as it could reveal that the primary obstacle isn't what you anticipate.<h4 id=scale-cost-effectively-for-load>Scale Cost-Effectively for Load</h4><p>Dividing our processing into individual microservices allows for independent scaling of each process. This approach potentially enables cost-effective scaling, focusing resources on scaling only the parts of our processing that currently limit our ability to handle loads. Additionally, we can scale down or deactivate microservices experiencing lower loads, optimising resource usage as needed.<p><strong>However you could...</strong><p>Scale vertically and just <em>get a bigger box</em> for a quick, short-term improvement. Another option to explore is horizontal scaling of the existing monolith, which is a relatively simple step to take and should be considered before investigating microservices for this purpose.<h4 id=improve-robustness>Improve Robustness</h4><p>Segmenting our application into independently deployable processes offers various mechanisms to enhance the resilience of our applications. With this approach, an issue in one area of functionality doesn't necessarily affect the entire system. We can prioritise our efforts on the parts of the application requiring the most robustness. Microservices don't inherently guarantee robustness; however, they provide opportunities to design a system that can better withstand network partitions, service outages, and similar challenges.<p><em>Robustness</em> refers to a system's capacity to respond to anticipated variations, while <em>resilience</em> involves an organisation's capability to adapt to unforeseen circumstances. For instance, anticipating a potential machine failure, we enhance system robustness by introducing redundancy through load balancing. Resilience, however, encompasses the organisational process of preparing for unforeseeable challenges, acknowledging that not all potential problems can be predicted.<p><strong>However you could...</strong><p>Deploy multiple copies of the monolith, possibly behind a load balancer or another load distribution mechanism such as a queue. In doing so we introduce redundancy to our system - improving robustness.<h4 id=scale-the-number-of-developers>Scale the Number of Developers</h4><p>In 'The Mythical Man-Month', Frederick Brooks argues that adding more people to a project can improve delivery only if the work can be divided into distinct tasks with minimal interdependencies. He illustrates this concept with the analogy of harvesting crops: multiple harvesters can work simultaneously in a field because their tasks don't require constant interaction. However, software development differs in that tasks vary, and outputs from one task often serve as inputs for others.<p>By establishing clear boundaries and designing an architecture that minimises interdependencies between microservices, we create functionality that can be developed independently with a low degree of coupling. This approach aims to increase developer scalability by reducing delivery contention.<p><strong>However you could...</strong><p>An alternative approach could be to consider implementing a modular monolith: where-by different teams own each module, and as long as the interface with other modules remains stable, they could continue to make changes in isolation.<h4 id=embrace-new-technology>Embrace New Technology</h4><p>Monoliths typically constrain our technological options, binding us to a single programming language, deployment platform, operating system, and database. Conversely, a microservice architecture offers the flexibility to diversify these choices for each service. By confining technological changes within individual service boundaries, we can evaluate the benefits of new technologies in isolation and mitigate the impact if issues arise.<p>Mature microservice organisations often restrict the number of technology stacks they support. However, the ability to experiment with new technologies safely can provide them with a competitive edge, both in terms of delivering superior results for customers and in fostering developer satisfaction as they gain expertise in new skills.<p><strong>However you could...</strong><p>We could easily embrace new languages within the same runtime environment; for instance, the JVM can seamlessly accommodate code written in multiple languages within a single running process. However, integrating new types of databases poses greater challenges, as it often requires decomposing a previously monolithic data model to facilitate an incremental migration.<h3 id=microservices-could-be-a-bad-choice-when-you>Microservices could be a bad choice when you...</h3><h4 id=have-an-unclear-domain>Have an unclear domain</h4><p>Incorrectly defining service boundaries can result in significant costs. It may necessitate a higher frequency of cross-service modification, create overly interdependent components, and, overall, prove to be more detrimental than maintaining a single monolithic system.<p>Decomposing a system into microservices prematurely can incur significant costs, particularly for those unfamiliar with the domain. It's generally easier to transition an existing codebase into microservices than to start with microservices from scratch.<h4 id=are-a-startup>Are a Startup</h4><p>Microservices are particularly beneficial for startups transitioning into scale-ups, tackling challenges that arise post-initial success. It's easier to partition an existing <em>brownfield</em> system than to do so upfront with a new <em>greenfield</em> system. Focus on success initially, splitting only around clear boundaries and maintaining a more monolithic approach for the rest. Startups often experiment with various ideas to find a fit with customers, which can result in significant shifts in the product domain (an unclear domain, as above).<h4 id=customer-installed-and-managed-software>Customer-Installed and Managed Software</h4><p>If your software is packaged and shipped to customers for self-operation, microservices might not be the most suitable option. Adopting a microservice architecture shifts significant complexity to the operational domain. It's unrealistic to assume that customers will possess the necessary skills or platforms to manage microservice architectures.<h4 id=do-not-have-a-good-reason>Do not have a good reason!</h4><p>The primary reason to avoid adopting microservices is the lack of a clear understanding of your objectives. Simply following the trend without a solid purpose is unwise and can lead to unfavourable outcomes.<h4 id=focus-on-reuse>Focus on reuse</h4><p>Making reuse a primary objective for microservice migration is often considered a misstep. It's not a direct outcome that people desire; rather, it's something they hope will lead to other benefits. While we anticipate that reuse may accelerate feature delivery or reduce costs, it's essential to monitor these primary objectives directly rather than fixating solely on reuse. Otherwise, there's a risk of optimising the wrong aspect of the development process.<h3 id=importance-of-incremental-migration>Importance of Incremental Migration</h3><blockquote><p>"If you do a big-bang rewrite, the only thing you‚Äôre guaranteed of is a big bang" - Martin Fowler</blockquote><p>It's recommended to gradually dismantle monoliths, extracting them bit-by-bit. An incremental approach allows for learning about microservices iteratively. Attempting a complete overhaul at once can hinder effective feedback on what's working well or not. Embrace the likelihood of making mistakes and focus on minimising their impact.<p>Each step provides valuable insights. Start with small areas of functionality, implement them as microservices, deploy them into production, and assess their effectiveness. It's crucial to understand that the extraction of a microservice isn't finalised until it's actively utilised in production.<h3 id=cost-of-change>Cost of Change</h3><blockquote><p>"Some decisions are consequential and irreversible or nearly irreversible - <em>one-way doors</em> - and these decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don‚Äôt like what you see on the other side, you can‚Äôt get back to where you were before. We can call these <em>Type 1</em> decisions. But most decisions aren‚Äôt like that - they are changeable, reversible‚Äîthey‚Äôre two-way doors. If you‚Äôve made a suboptimal <em>Type 2</em> decision, you don‚Äôt have to live with the consequences for that long. You can reopen the door and go back through. Type 2 decisions can and should be made quickly by high judgment individuals or small groups." - Jeff Bezos</blockquote><p>Mistakes are inevitable and should be embraced, but it's crucial to understand how to minimize their costs. The terms 'Type 1' and 'Type 2' may not be descriptive enough, so using 'Irreversible' for Type 1 and 'Reversible' for Type 2 can provide clarity. Viewing Irreversible and Reversible decisions as two ends of a spectrum helps gauge their impact on potential course corrections. Planning often takes place on the whiteboard, where the cost of change and mistakes is minimal. Sketching out the proposed design and testing various use cases across potential service boundaries can provide valuable insights. When it comes to decomposing an existing monolithic system, we need to have some form of logical decomposition to work with, and this is where <em>domain-driven design</em> can come in handy.<h3 id=using-a-domain-model>Using a Domain Model</h3><p>Bounded contexts are excellent starting points for defining microservice boundaries. A domain model provides a logical view of an existing system, but there is no guarantee that the underlying existing code structure aligns with this model. Additionally, a domain model won't indicate which bounded contexts store data in a given database for example. Despite this, developing a domain model is crucial for structuring a microservice transition. It's important to gather just enough information from the domain model to make informed decisions about where to begin the decomposition.<p>For example, Invoicing might seem like an easy initial step, if our aim is to enhance time to market and the Invoicing feature seldom undergoes changes, it might not be the most productive use of our time.<p>We seek quick wins for early progress, momentum, and feedback, which leads us to choose easier components to extract. However, we also need to ensure that the decomposition provides meaningful benefits. Balancing these considerations is crucial in our decision-making process.<h4 id=event-storming>Event Storming</h4><p>Event Storming, created by Alberto Brandolini, is a collaborative exercise where technical and non-technical stakeholders come together to define a shared domain model. This process starts from the <em>bottom up</em>, with participants identifying 'Domain Events' - things that happen in the system. These events are then grouped into aggregates, which are subsequently organised into bounded contexts. The goal of Event Storming is to understand the logical events occurring in the system, rather than to build an event-driven system. The value lies not only in the model itself but also in the shared understanding developed through this collective exercise.<h3 id=reorganising-teams>Reorganising Teams</h3><p>Aligning architecture and organisation can be pivotal in maximising the benefits of a microservice architecture. Traditionally, IT organisations were structured based on core competencies, with backend/frontend developers, testers, and DBAs each forming separate teams. This setup often led to multiple hand-offs between teams during software development and deployment, resulting in delays. Similarly, organisational silos could impede efficiency, as involving multiple teams in software creation or changes could prolong the process.<p>These silos have been breaking down. Rather than concentrating on specific technologies or activities, teams focus is now on different aspects of the product, mirroring the shift from technical-oriented services to services organised around vertical slices of business functionality.<p>The DevOps movement has prompted many organisations to move away from centralised operations teams, empowering delivery teams with greater responsibility for operational considerations. The roles of centralised teams have evolved significantly. Previously focused on executing tasks themselves, these teams now primarily support and enable delivery teams. This support may involve embedding specialists within teams, developing self-service tooling or providing training. Their responsibility has transitioned from execution to facilitation, fostering the emergence of more independent and autonomous delivery teams.<p>But remember! Draw inspiration from the experiences of other organisations, but avoid assuming that their solutions will seamlessly apply to your context. As Jessica Kerr stated regarding the Spotify model (from the well known 'Scaling Agile' paper), focus on replicating the underlying questions rather than blindly adopting their answers. Ensure that any changes you implement are grounded in a deep understanding of your company, its requirements, its personnel, and its cultural nuances.<h3 id=how-will-you-know-if-the-transition-is-working>How Will You Know if the Transition Is Working?</h3><ul><li><strong>Regular checkpoints</strong> - set up checkpoints to provide your team with opportunities to evaluate whether you're progressing in the desired direction.<li><strong>Quantitative measurements</strong> - numerical data or metrics employed to assess and evaluate particular facets of a system, process, or result. The metrics chosen for monitoring progress depend on the objectives being pursued. However, it's important to recognise that metrics can have pitfalls due to the saying 'You get what you measure'. Metrics can be manipulated, either unintentionally or intentionally.<li><strong>Qualitative measurements</strong> - Derived from subjective observations, descriptions, or interpretations. Regardless of the data at hand, software development is a human-driven endeavour, underscoring the importance of incorporating feedback from those directly involved in measuring success.</ul><h3 id=avoiding-the-sunk-cost-fallacy>Avoiding the Sunk Cost Fallacy</h3><p>The Sunk Cost Fallacy, also termed the Concorde Fallacy, refers to the tendency for individuals to continue investing resources (time, money, effort) into a project or decision, even if it's clear that the expected outcome is unlikely to justify the additional investment. This behaviour occurs because people focus on the resources already invested ('sunk costs') rather than objectively assessing the future benefits and costs of continuing or discontinuing the endeavour.<p>Breaking down tasks into smaller steps helps mitigate the sunk cost fallacy and facilitates easier course adjustments. Utilising the checkpoint mechanism enables reflection on progress. While it's not necessary to change direction immediately at the first sign of trouble, disregarding evidence of success or failure is riskier than not gathering evidence at all.<h2 id=splitting-the-monolith>Splitting the Monolith</h2><p>We want to transition to a microservice architecture incrementally, allowing us to learn and adapt as needed, without a complete rewrite. To do this, we have to ensure that we can continue to work with, and make use of our existing monolithic applications. If we can modify the current system, it offers the most flexibility. However, constraints like lack of source code, obsolete technology skills, or high modification costs might limit this. Additionally, the monolith could be in poor condition or heavily used by others, complicating changes.<p>When migrating functionality to new microservices, it's not always clear what to do with the existing code. Should we move the code as is or re-implement the functionality? If the monolithic codebase is well-structured, moving the code itself can save significant time.<h3 id=refactoring-the-monolith>Refactoring the Monolith</h3><p>Experience has shown that the biggest barrier to using existing monolithic code in new microservices is that monolithic codebases are typically not organised around business domain concepts. Instead, they are categorised technically (e.g. Model, View, Controller). This makes it challenging to move business domain functionality, as it can be difficult to locate the code to move.<p>In the book 'Working Effectively with Legacy Code' Michael Feathers defines a seam as a place where you can change a program's behaviour without editing the existing behaviour. Essentially, you define a seam around the code you want to change, develop a new implementation, and then swap it in. While Michael‚Äôs concept of seams can be applied broadly, it fits particularly well with bounded contexts.<h4 id=modular-monolith>Modular monolith</h4><p>After understanding the existing codebase, you could consider extracting the newly identified seams as separate modules to create a modular monolith. This approach allows for independent module development, providing many benefits while avoiding many of the challenges of a microservice architecture.<h4 id=incremental-rewrites>Incremental rewrites</h4><p>In practice it is found that few teams refactor their monoliths before transitioning to microservices. More commonly, teams identify the responsibilities of the new microservice and create a <em>clean-room</em> implementation of that functionality. This approach risks the problems of big bang rewrites. To avoid this, rewrite only small pieces of functionality at a time and regularly deliver the updates to customers. If reimplementing the service takes a few days or weeks, it's manageable. But if it starts taking several months, it's time to reassess the approach.<h3 id=pattern-strangler-fig-application>Pattern: Strangler Fig Application</h3><p><a href=https://martinfowler.com/bliki/StranglerFigApplication.html rel=noopener target=_blank>ref</a><p>One widely-used technique for system rewrites is known as the <em>strangler fig application</em>. Inspired by a type of fig tree that starts growing on a host tree's upper branches and gradually replaces it, this approach in software involves the new system initially wrapping around and relying on the existing one. Over time, the new system can grow independently until it completely supplants the old one. This method supports incremental migration to a new system, allowing for pauses in the process while still utilising the progress made so far. Each step in this approach is designed to be reversible, minimising the risks associated with incremental changes.<p>The pattern is comprised of three steps:<ul><li>Identify parts of the existing system to migrate. <ul><li>You may consider extracting larger groups of functionality to streamline the process, but this approach involves a delicate balance. Extracting larger slices increases workload while potentially easing integration challenges.</ul><li>Implement this functionality in a new microservice. <ul><li>This may involve actually copying the code from the monolith (if possible), or else reimplementing the functionality in question.</ul><li>Reroute calls from the monolith over to the new microservice. <ul><li>You need to be able to clearly map the inbound call to the functionality you wish to change.<li>HTTP is very amenable to redirection, for example, URI path redirection.<li>Your proxy could also transform the protocol i.e. SOAP-based HTTP interface remapped to gRPC. However this puts <em>smarts</em> in the pipes, which are preferred to be placed in the clients. Instead push the mapping into the service itself, as a shared proxy layer can slow down the process of making and deploying changes.<li>Use of a service mesh can help remove the shared proxy layer, with each service using its own dedicated proxy.<li>Message-driven architectures (i.e. through a message broker) can use a content-based router to send messages to the relevant system. So as to not make a <em>smart</em> pipe, both services could receive all messages and ignore any they do not wish to handle. Coordination between handling changes is required between the two systems which can be tricky to orchestrate (over a shared router).<li>Feature-toggles can be used to activate the redirection.</ul></ul><p><strong>Note:</strong> until the call to the moved functionality is redirected, the new functionality isn‚Äôt technically live, even if it is deployed into a production environment.<p>Separating <em>deployment</em> from <em>release</em> is important as it allows validation in the production environment before customer use, minimising rollout risks.<p>Allowing changes to functionality during the migration complicates potential rollbacks. It's simpler to avoid changes until migration is fully complete. The longer the migration process, the more challenging it becomes to enforce a 'feature freeze' in that system area though. Smaller migrations reduce the urgency to modify functionality before completing the process.<h3 id=pattern-ui-composition>Pattern: UI Composition</h3><p>The user interface offers valuable opportunities to integrate functionality from both the existing monolith and new microservices. UI composition effectively facilitates system re-platforming by enabling migration of complete vertical slices of functionality. It does however require the ability to modify the existing user interface to incorporate said functionality. This can be done achieved using:<ul><li><strong>Page Composition</strong> - Allows us to switch functionality at the entire page level.<li><strong>Widget Composition</strong> - Allows us to switch functionality within a page. <ul><li>This can be achieved with ESI's or use of micro-frontends within SPA's.<li>Micro-frontends involve dividing a user interface into separate components that can be developed and deployed independently (similar to microservices).<li>Web Components is standard which can be used, however has slow adoption.</ul></ul><h3 id=pattern-branch-by-abstraction>Pattern: Branch by Abstraction</h3><p><a href=https://martinfowler.com/bliki/BranchByAbstraction.html rel=noopener target=_blank>ref</a><p>For the strangler fig pattern to be effective, we need to intercept calls at the perimeter of our monolith. However, if the functionality we aim to extract is deep within the existing system, significant changes are necessary. These changes can disrupt other developers working concurrently on the codebase. To balance these tensions, developers typically work on separate source code branches when reworking parts of the codebase. This approach minimises disruption but introduces challenges when merging changes back into the main branch. The <em>branch by abstraction pattern</em> offers an alternative by modifying the existing codebase to safely integrate new implementations alongside existing ones in the same version, reducing disruption to other developers while allowing for incremental changes.<p>Branch by abstraction consists of five steps:<ul><li>Create an abstraction for the functionality to be replaced.<li>Change clients of the existing functionality to use the new abstraction.<li>Create a new implementation of the abstraction with the reworked functionality. In our case, this new implementation will call out to our new microservice.<li>Switch over the abstraction to use our new implementation.<li>Clean up the abstraction and remove the old implementation.</ul><p>There is a variation of this pattern called <em>verify branch by abstraction</em>, where-by if calls to the new implementation fail, then the system is able to automatically fallback to the old implementation.<h3 id=pattern-parallel-run>Pattern: Parallel Run</h3><ul><li>Call both the old and new implementations simultaneously and compare their results.<li>Only one serves as the source of truth - usually the old one until the new one is verified<li>Ensures the new implementation meets functional and nonfunctional requirements, such as response time and error rates.<li>You can use a Spy (from unit testing) to assert behaviour of a service without incurring additional side-effects i.e if you wish to parallel run a Notification service, we would not want to send out two emails.<li>Where you position the Spy is important. If you place it within the monolith we do not account for the impact of the remote call (such as timeouts, failures, latency) so we test less. However it makes the verification step easier as it is all within the same process.<li>It is recommended to have the Spy as close to the delimitating factor as possible, so as to test as much of the behaviour as you can. This requires out-of-process verification to be performed (typically on a schedule).<li>Parallel Run is a form of <em>Progressive Delivery</em>, which focuses on gradual and controlled rollout of new features. This minimises risk and ensures stability during deployment. Other variants of this approach are: <ul><li>Canary Releasing - directs a subset of users to the new functionality.<li>Dark Launching - deploys and tests new functionality invisibly to users.</ul></ul><h3 id=pattern-decorating-collaborator>Pattern: Decorating Collaborator</h3><p>This approach enables triggering new behaviour within the monolith without changing it, using the decorator pattern to simulate direct calls from the monolith to our services. Instead of intercepting calls, we let them proceed normally and then call external microservices based on the results through mechanisms like proxies. This pattern works best when necessary information can be extracted from the inbound request or the monolith's response, but becomes complex when additional information is needed for the new service. Such as requiring to fetch more information from the monolith to complete the microservices action.<h3 id=pattern-change-data-capture>Pattern: Change Data Capture</h3><p>With change data capture, rather than trying to intercept and act on calls made into the monolith, we react to changes made in a datastore. For change data capture to work, the underlying capture system has to be coupled to the monolith‚Äôs datastore. There are several ways of reacting to these changes within your microservices:<ul><li><strong>Database triggers</strong> - special types of stored procedures that automatically execute or 'trigger' in response to certain events on a particular table.<li><strong>Transaction log pollers</strong> - continuously monitor the out-of-process transaction log of a database to capture and react to changes as they happen.<li><strong>Batch delta copier</strong> - regularly schedule scans of database to find (and react to) the data that has changed.</ul><p>This approach is suited when you need to react to data changes in the monolith but cannot intercept these changes at the system's perimeter (strangler or decorator) or modify the underlying codebase.<h2 id=decomposing-the-database>Decomposing the Database</h2><p>Microservices work best when we practice information hiding, which means having their own data store. With a shared database it can be unclear as to who 'controls' the data and where the business logic that manipulates this data resides.<p>Shared databases are considered appropriate within a microservice architecture only for:<ul><li>Read-only static reference data.<li>Situations where a service directly exposes a database as a defined endpoint, designed and managed to accommodate multiple consumers (as discussed in several of the patterns below).</ul><h3 id=pattern-database-view>Pattern: Database View</h3><p>To provide a single source of data for multiple services while mitigating coupling concerns, a read-only view can be used. This view presents a service with a limited projection from the underlying schema, implementing a form of information hiding. Database views should be maintained by the team managing the source schema, similar to other service interfaces. This approach is limited to query results and requires services to have access to the source database.<h3 id=pattern-database-wrapping-service>Pattern: Database Wrapping Service</h3><p>The database wrapping service pattern hides the database behind a thin service wrapper, converting database dependencies into service dependencies. This allows control over what is shared and hidden, which is useful when the underlying schema is too complex to separate. By enforcing access only through the wrapping service, further growth of the database can be controlled and ownership boundaries clarified. Unlike simple database views, this pattern allows for more sophisticated data projections and supports write operations via API calls. However, it does require consumers to switch from direct database access to using the API.<h3 id=pattern-database-as-a-service-interface>Pattern: Database-as-a-Service Interface</h3><p><a href=https://martinfowler.com/bliki/ReportingDatabase.html rel=noopener target=_blank>ref</a><p>Sometimes, clients need a database to query for large data sets because their tools require a SQL endpoint. For this we can create a dedicated, read-only database that is updated whenever the underlying database changes. This is similar to exposing a stream of events or a synchronous API. An mapping engine is used as an abstraction layer between internal and external databases, ensuring the public-facing database remains consistent even if the internal database structure changes.<h3 id=pattern-aggregate-exposing-monolith>Pattern: Aggregate Exposing Monolith</h3><p>Microservices should be viewed as a combination of behaviour and state, similar to aggregates. When exposing an aggregate from the monolith, the monolith retains control over permitted state changes, acting as more than just a database wrapper. This approach involves exposing operations that let external parties query the current state of an aggregate and request new state transitions. We can restrict which aspects of an aggregate's state are exposed and limit which state transition operations can be requested externally.<p>When the data is still 'owned' by the monolith, this pattern effectively provides your new services with the necessary access. For extracted services, having the new service call back to the monolith for data access is usually not much more work than directly accessing the monolith's database, but it is a better long-term solution (over Database Views, for example).<h3 id=pattern-change-data-ownership>Pattern: Change Data Ownership</h3><p>If the microservice has assumed responsibility for that behaviour or data, we should invert the dependency, making the monolith treat the new service as the source of truth. This means the monolith will call the microservice's endpoint to read data and request changes.<h3 id=pattern-synchronise-data-in-application>Pattern: Synchronise Data in Application</h3><p>The strangler fig pattern offers the flexibility to revert back to the monolith if issues arise after transitioning to the new service. However, challenges can occur when the new service must maintain data synchronisation with the monolith.<p>This approach ensures database synchronisation through code, with either the monolith or the new service handling writes to both databases. It works best when either the new service or the monolith exclusively manages writes at any given time, which is conducive to a straightforward switchover strategy like the strangler fig pattern. However, during scenarios like canary deployments where requests might interact with both the monolith and the new service concurrently, maintaining synchronisation becomes complex, making this pattern less suitable.<h3 id=pattern-tracer-write>Pattern: Tracer Write</h3><p>With the Tracer write pattern, the data's source of truth is gradually shifted, allowing for coexistence of two sources during migration. A new service is chosen to host the relocated data, while the current system maintains a local record. Updates are synchronised by ensuring changes are mirrored to both the current system and the new service through its service interface. Existing code is then adapted to exclusively utilise the new service. Once all functionality relies on the new service as the primary source, the old source can be phased out.<h3 id=transactions>Transactions</h3><p>A transaction is a sequence of operations performed as a single, indivisible unit of work. Transactions ensure that all operations succeed or none do, maintaining integrity and consistency, which simplifies system development and maintenance.<h4 id=acid-transactions>ACID Transactions</h4><p>Typically, when we talk about database transactions, we talk about ACID transactions.<ul><li><strong>Atomicity</strong>: <ul><li>All operations within a transaction complete together or not at all.<li>If any change fails, the entire transaction is aborted.<li>Atomicity is often the first issue encountered when splitting transactional boundaries.</ul><li><strong>Consistency</strong>: <ul><li>Ensures database remains in a valid, consistent state after changes</ul><li><strong>Isolation</strong>: <ul><li>Multiple transactions can operate concurrently without interference<li>Interim state changes are invisible to other transactions</ul><li><strong>Durability</strong>: <ul><li>Completed transactions are permanent and resilient to system failures</ul></ul><p>When we split our database apart we can still use ACID transactions, however, the scope of these transactions is reduced to a specific service - not across services.<h4 id=distributed-transactions-two-phase-commit>Distributed Transactions: Two-Phase Commit</h4><p>The two-phase commit (2PC) algorithm is used for transactional changes in distributed systems, involving two phases: voting and commit. During the voting phase, a coordinator checks if all workers can perform a state change. If any worker cannot, the operation aborts. If all agree, the commit phase begins, and changes are made.<p>There are several limitations to this approach:<ul><li>Cannot guarantee simultaneous commits, causing potential inconsistencies.<li>Requires locking resources, increasing the risk of deadlocks.<li>Numerous failure modes, some needing manual resolution.<li>High latency and complexity, especially with many participants or long transactions.</ul><p>This means of distributed transaction should be avoided when coordinating state changes across microservices due to complexity and above limitations. If possible, avoid splitting data that requires atomic and consistent management. Keep such data in a single database and manage it within a single service or monolith. If data must be separated across services look to the below Saga pattern instead.<h4 id=distributed-transactions-saga>Distributed Transactions: Saga</h4><p>Unlike a two-phase commit, a saga coordinates multiple state changes without long-term resource locking. It achieves this by modelling steps as independent activities, promoting explicit business process modelling. Originally proposed for long-lived transactions (LLTs), sagas split LLTs into shorter, independent (ACID) transactions to minimize database contention. This approach extends to coordinating changes across multiple services, breaking down business processes into sequential calls within a single saga. It's crucial to note that while sagas lack the atomicity of traditional ACID transactions across the entire saga, each sub-transaction maintains ACID properties individually, enabling reasoned state management.<p>In sagas, handling failures involves two recovery approaches: <em>backward recovery</em> and <em>forward recovery.</em> Backward recovery reverts failures through compensating actions that undo committed transactions. Forward recovery retries transactions from the failure point, ensuring that the entire saga progresses towards completion despite encountering possible transient failures.<p>It's practical to employ varied failure recovery approaches. Certain failures may necessitate rollback (backward recovery), while others can proceed forward (forward recovery). For example: In the context of order processing, after receiving payment and packaging the item, the final step is dispatching. If dispatch fails due to operational reasons, like a lack of delivery capacity, rolling back the entire order seems unnecessary. Instead, retrying the dispatch and involving human intervention if retries fail is a more logical approach to resolve the issue.<h5 id=orchestrated>Orchestrated</h5><p>Orchestrated sagas employ a central coordinator, or orchestrator, to dictate the sequence of operations and trigger compensating actions when needed (similar to a musical conductor). This command-and-control approach provides clear visibility into saga execution. Explicitly modelling business processes within the orchestrator enhances system understanding and facilitates onboarding. However, this approach increases domain coupling, as the orchestrator must be aware of all involved services. It also risks centralising logic that should ideally reside within individual services, potentially leading to service anemia. To mitigate these issues, consider distributing orchestration responsibilities among different services for different workflows, promoting service autonomy and reuse of functionality across various business processes.<h5 id=choreographed>Choreographed</h5><p>Choreographed sagas distribute responsibility among multiple collaborating services, contrasting with the command-and-control nature of orchestrated sagas. It is analogous to a dance, where-by everyone knows what they need to do - communicating and moving with each other, there is no single conductor telling them what to do. Services communicate through events broadcasted throughout the system, where each service reacts autonomously to events it receives without direct knowledge of other services. This decentralised approach reduces coupling and avoids centralised logic. However, understanding the overall business process becomes challenging without an explicit model. Additionally, tracking saga state for necessary actions like compensating transactions becomes complex without a central coordinator. To address this, systems can use correlation IDs in events, allowing a dedicated service to track saga states and manage necessary actions programmatically.<h2 id=growing-pains>Growing Pains</h2><p>Don‚Äôt think of adopting microservices as flipping a switch; think about it as turning a dial. As you increase your use of microservices, you'll unlock their benefits but also encounter new challenges. Addressing these requires fresh thinking, new skills, different techniques, and possibly new technology.<h3 id=code-ownership-at-scale>(Code) Ownership at Scale</h3><p><strong>Strong code ownership</strong><p>Every service has designated owners. Changes by non-owners must be submitted to the owners for approval. Using pull requests is one way to manage this process.<p>Strong code ownership is common in large-scale microservice architectures with multiple teams and over 100 developers. This model promotes product-oriented teams, allowing them to focus on specific business domains. Such teams maintain customer focus, build domain expertise, and often work with embedded product owners.<p><strong>Weak code ownership</strong><p>Most services have designated owners, but anyone can still make direct changes. Source control allows open access, with the expectation that changes to others' services are discussed with the owners beforehand.<p><strong>Collective code ownership</strong><p>No one owns anything, and anyone can change anything they want. For collective ownership to work, the team must be well-connected and share a common understanding of good changes and the technical direction for each service.<p>For small teams, collective code ownership works well. However, for rapidly growing teams, it becomes problematic. Collective ownership requires time for consensus to form and evolve, which is difficult with increasing team size and rapid hiring.<h3 id=breaking-changes>Breaking Changes</h3><p>A microservice is part of a larger system, consuming and/or exposing functionality. We aim for independent deployability, ensuring changes don't break consumers, with this exposed functionality acting as a <em>contract</em>.<p><strong>Eliminate accidental breaking changes</strong><p>An explicit schema for your microservice helps detect <em>structural breakages</em>. For example, changing a method from taking two integers to one is an obvious breaking change. <em>Semantic breakages</em> can also occur however; if a method that added two integers now multiplies them, it breaks the contract (expected behaviour) as well. Make contract changes obvious to developers with explicit schemas and thorough test coverage.<p><strong>Think twice before making a breaking change</strong><p>Prefer expanding your contract by adding new methods or resources without removing the old ones. Support both old and new functionality whenever possible. This provides you with a means of you and your consumers gracefully moving over to the revised contract behaviour.<p><strong>Give consumers time to migrate</strong><p>Microservices should be independently deployable. Changes must not impact existing consumers, allowing them to use the old contract while the new one is available. Give consumers time to migrate to the new version by either running two versions side-by-side or, preferably, supporting both contracts within a single version.<h3 id=reporting>Reporting</h3><p>In a monolithic system, a single database makes data analysis straightforward with a ready-made schema for reporting. Microservice architecture scatters data across isolated schemas, complicating these efforts. Stakeholders often rely on tools and processes that require direct SQL access to a single database, usually tied to the schema design of your monolithic database. To avoid disrupting their workflow, you may need to present a single database for reporting, potentially matching the old schema to minimize changes. If your monolith uses a dedicated data source like a data warehouse or data lake, ensure microservices copy data there to maintain reporting.<h3 id=monitoring-and-troubleshooting>Monitoring and Troubleshooting</h3><blockquote><p>‚ÄúWe replaced our monolith with microservices so that every outage could be more like a murder mystery.‚Äù Honest Status Page (@honest_update)</blockquote><p>Monitoring a monolithic application is straightforward: with fewer machines and a binary failure mode, the system is either up or down. High CPU usage (e.g. 100%) clearly indicates a problem. In contrast, a microservice architecture involves numerous services and instances, where a single service failure doesn't necessarily mean the whole system is down. Therefore, the level of importance of a 100% CPU could be deemed a lower severity in this case.<p><strong>Log aggregation</strong><p>With a few long-lived machines, logs could be fetched directly from the machines. However, in a microservice architecture with many more, often short-lived processes, a log aggregation system is essential. It captures and forwards all logs to a central location for searching and alert generation.<p><strong>Tracing</strong><p>Analysing call sequences and identifying the source of failures or latency spikes in microservices is challenging when viewing each service in isolation. Collating these flows is invaluable, this can be achieved by generating correlation IDs for all incoming calls and tying the different service requests together using it. Along with logs, we can also trace the time taken for calls etc. using distributed tracing tools like Jaeger.<p><strong>Testing in production</strong><p>Functional automated tests provide feedback on software quality before deployment, but we need ongoing feedback in production. New deployments or environmental changes can break features that previously worked. By using <em>synthetic transactions</em> to simulate user behaviour, we can define expected outcomes and set up alerts for failures. Start by adapting existing end-to-end test cases for production use, ensuring these tests don't adversely impact the production environment.<p><strong>Observability</strong><p>Traditional monitoring and alerting focus on predicting known issues, like disk-space running out, unresponsive services, or latency spikes. However, as systems become more complex, it's harder to foresee all potential problems. Therefore, it's crucial to enable open-ended inquiries into our systems when issues arise, to quickly stabilize operations and gather data for future fixes. Comprehensive data collection, including tracing and logs, allows us to investigate unexpected problems using real information rather than conjecture.<h3 id=local-developer-experience>Local Developer Experience</h3><p>As the number of services grows, the developer experience can deteriorate. Resource-intensive runtimes like the JVM limit the number of microservices that can run on a single machine, slowing down local builds and execution. Developers may request more powerful machines, but this is only a temporary fix if services continue to grow. Teams practicing collective ownership of multiple services are more affected, as they often need to switch between services. In contrast, teams with strong ownership of fewer services focus on their own and develop methods to stub out external services.<p>To reduce the number of locally run services, developers commonly stub out unnecessary services or point to instances running elsewhere. A fully remote setup can leverage more capable infrastructure for numerous services. Monitoring how the developer experience evolves with increasing services is crucial. Implement feedback mechanisms and invest continuously to maintain developer productivity.<h3 id=running-too-many-things>Running Too Many Things</h3><p>As the number of services and instances grows, managing deployments and configurations becomes more complex. Traditional methods for handling monolithic applications often don't scale well with numerous microservices. Desired state management becomes crucial. This involves specifying and maintaining the required number and location of service instances. Manual processes might work for monoliths but won't scale with tens or hundreds of microservices, each with different desired states. This is when you should look into tooling such as Kubernetes or follow a Serverless-first mindset and go with FaaS offerings such as AWS Lambda.<h3 id=end-to-end-testing>End-to-End Testing</h3><p>When employing automated functional tests, there's a delicate balance to maintain. The broader the functionality tested, the greater the confidence in your application, but this also increases test duration and complexity in diagnosing failures. In a microservice architecture, end-to-end tests span multiple services, requiring deployment and configuration across these services, and encountering challenges like false negatives due to environmental issues.<p>To manage this effectively:<ul><li><strong>Limit Test Scope:</strong> Keep tests covering multiple services within the team managing those services to avoid crossing team boundaries.<li><strong>Consumer-Driven Contracts (CDCs):</strong> Use CDCs where consumers define service expectations via executable specifications (tests), ensuring these tests pass when service changes occur. Tools like Pact facilitate this approach.<li><strong>Automated Release and Progressive Delivery:</strong> Implement progressive delivery techniques like canary releases to incrementally roll out new software versions, monitoring key performance indicators (KPIs) to automate decision-making on continuing or rolling back deployments.<li><strong>Refine Quality Feedback Cycles:</strong> Continuously adapt testing strategies based on holistic insights into development processes. This involves adding new tests to cover emerging production issues and removing redundant tests to optimise feedback cycles.</ul><h3 id=global-vs-local-optimisation>Global vs. Local Optimisation</h3><p>Embracing decentralised decision-making within teams, including full lifecycle ownership of microservices, necessitates balancing local autonomy with global alignment. A common challenge arises when multiple teams independently address the same issue without realising it, leading to inefficiencies over time.<p>This issue typically surfaces in multi-team organisations, particularly those granting teams greater autonomy. It often becomes apparent after periods of rapid scaling, where increased developer numbers strain ad hoc information sharing, fostering information silos. Collective ownership of services can mitigate these challenges by promoting consistent problem-solving approaches.<p>Ultimately, this challenge revolves around distinguishing between reversible and irreversible decisions. Decisions with higher change costs require broader consensus. Teams must recognise when decisions lean towards irreversibility and involve stakeholders outside their team to ensure effective collective ownership and scalability.<h3 id=robustness-and-resiliency>Robustness and Resiliency</h3><p>In distributed systems, failure modes such as lost network packets, timeouts, and machine failures are common, contrasting with the relative simplicity of monolithic setups. These challenges are most pronounced in production environments, where replicating these conditions during testing is challenging. As the number of services and service calls grows, systems face heightened vulnerability to resilience issues like cascading failures and back pressure, particularly in tightly interconnected environments.<p><strong>Potential Solutions</strong><ul><li><strong>Understanding Service Calls:</strong> Begin by assessing potential failure scenarios for each service call and defining response strategies.<li><strong>Implementing Solutions:</strong> Strategies include isolating services to reduce interdependencies, employing asynchronous communication to prevent temporal coupling, setting sensible timeouts to manage slow services, and using patterns like circuit breakers for rapid failure mitigation.<li><strong>Operational Practices:</strong> Running multiple service instances and utilizing platforms with desired state management help mitigate risks associated with service failures.</ul><p>Resilience goes beyond pattern implementation; it involves cultivating an organisational culture prepared to address unforeseen challenges and adapt practices as needed. A practical step is documenting production issues promptly and learning from them.<h3 id=orphaned-services>Orphaned Services</h3><p>Organisations struggle with fundamental challenges in asset management, often uncertain about what they possess, its location, and responsible oversight. Microservices, when highly specialised, can persist without updates for long durations, leading to 'orphaned services' lacking active ownership. Addressing this requires implementing collective ownership practices, fostering shared responsibility among teams and utilising collaborative tools for effective service management.